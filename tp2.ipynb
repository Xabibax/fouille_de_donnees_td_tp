{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TP 2 : Fouille d’itemsets fréquents et de règles d’association sous Python\n",
    "\n",
    "IMT Atlantique – FIL A3\n",
    "Apprentissage Automatique\n",
    "\n",
    "## Objectifs :\n",
    "\n",
    "MLxtend (pour machine learning extensions) est une bibliothèque logicielle développée par Sebas-\n",
    "tian Raschka, qui propose des méthodes d’extraction de itemsets fréquents et de règles d’association\n",
    "basée sur Apriori. D’autres algorithmes plus sophistiqués sont aussi proposés.\n",
    "\n",
    "Ce TP se déroule en deux parties : découverte de MLxtend via des exemples 1 simples et application\n",
    "d’algorithmes de fouille de itemsets sur des données de vente au détail en ligne.\n",
    "\n",
    "### 1 Partie 1 : MLxtend pour la fouille d’itemsets\n",
    "\n",
    "### 1.1 Installation\n",
    "\n",
    "MLxtend est une bibliothèque Python qui implémente différentes méthodes pour l’apprentissage\n",
    "machine. Parmi ces méthodes, on retrouve les algorithmes d’extraction de itemsets fréquents, de\n",
    "maximaux fréquents et de règles d’association.\n",
    "\n",
    "L’installation de cette bibliothèque se fait via [canda](http://rasbt.github.io/mlxtend/installation/), avec la commande suivante : conda\n",
    "install mlxtend --channel conda-forge\n",
    "\n",
    "L’utilisation de fonctionnalités de MLxtend commence par l’importation de ces trois librairies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpmax\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###1.2 Importation et préparation des données\n",
    "\n",
    "### 1.2.1 Données transactionnelle\n",
    "\n",
    "Considérons le dataset ci-dessous décrivant les caddies de supermarché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [ ['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2.2 Transformation en tableau binaire\n",
    "\n",
    "La méthode apriori de la librairie MLxtend prend en entrée un tableau binaire où sont recensés\n",
    "la présence (codé True) ou l’absence (codée par False) des produits dans chaque caddie.\n",
    "\n",
    "— L’objet TransactionEncoder permet de transformer un ensemble de données en un tableau\n",
    "binaire ;  \n",
    "— Avec la méthode fit, le TransactionEncoder apprend les étiquettes uniques dans l’ensemble de données, et via la méthode de transformation, il transforme l’ensemble de données d’entrée (une liste de listes) en un tableau de booléens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "TB = TransactionEncoder()\n",
    "TBA = TB.fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Le résultat de cette étape est un tableau NumPy au format ndarray. Il est possible de transformer\n",
    "ce tableau en un dataframe grâce à Pandas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame (TBA, columns=TB.columns_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Extraction d’itemsets fréquents et des maximaux\n",
    "\n",
    "Pour extraire les itemsets fréquents il faut appliquer la fonction apriori() au dataframe et\n",
    "fixer la valeur du support minimum (dans notre cas, min support=0.5). Il est aussi possible de\n",
    "contraindre la taille des itemsets retournés avec le paramètre max len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "FI = apriori(df,min_support=0.5, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Les résultats sont stockés dans une structure de type ”pandas/DataFrame”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "type(FI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Elle est composée de 2 colonnes : le support et la description des itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "print(FI.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pour nos données et avec les paramètres ci-dessus, vous devez obtenir 11 itemsets fréquents suivants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "print(FI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "On souhaite à présent mesurer le temps d’exécution de Apriori pour extraire les itemsets\n",
    "fréquents. Pour cela, nous allons utiliser la fonction magique ipython %timit, qui peut être utilisée\n",
    "pour chronométrer un morceau de code particulier (une seule instruction d’exécution ou une seule\n",
    "méthode).\n",
    "\n",
    "##### Note\n",
    "Usage, in line mode :\n",
    "%timeit -n<N> -r<R> [-t|-c] -q -p<P> -o] statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%timeit -n 100 -r 10 apriori(df, min support=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "La librairie MLxtend propose d’autres algorithmes sophistiqués pour extraire les itemsets fréquents,\n",
    "comme l’algorithme fpgrowth.\n",
    "\n",
    "##### Question\n",
    "Appliquez l’algorithme fpgrowth pour extraire les itemsets fréquents, puis comparez les temps\n",
    "de calculs avec Apriori. Que pouvez-vous conclure ?\n",
    "\n",
    "Pour extraire les itemsets fréquents maximaux, il faut utiliser la fonction fpmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "MFI = fpmax(df,min_support=0.5, use_colnames=True)\n",
    "print(MFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4Extraction de règles d’association\n",
    "\n",
    "La fonction association rules de MLxtend prend les dataframes des itemsets fréquents pro-\n",
    "duits par les fonctions apriori(), fpgrowth() ou fpmax().\n",
    "\n",
    "Pour limiter le nombres de règles extraites, la fonction permet de spécifier (1) la métrique\n",
    "d’intérêt (paramètre metric) et (2) le seuil correspondant (paramètre min threshold). Les mesures\n",
    "actuellement mises en œuvre sont la confiance (metric = ”confidence”) et le lift (metric = ”lift”).\n",
    "\n",
    "Le code ci-dessous illustre un exemple d’extraction de règles d’association à partir de itemsets\n",
    "fréquents avec un niveau de confidence d’au moins 80% (min threshold=0.8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "AR = association_rules(FI,metric=\"confidence\",min_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Question\n",
    "Affichez pour chaque règle l’antécédent et son le conséquent.\n",
    "\n",
    "### 2 Partie 2 : Application à des données de vente en détail\n",
    "\n",
    "Nous allons maintenant appliquer les différents algorithmes vus précédemment sur des données\n",
    "de vente en détail en ligne issu de la base retail. Le fichier ”retail dataset.csv ” est un\n",
    "fichier au format csv qui se présente sous la forme d’une base transactionnelle ou les transactions\n",
    "représentent des caddies de supermarché : chaque ligne correspond aux noms des produits.\n",
    "\n",
    "### 2.1 Évaluation\n",
    "\n",
    "Vous devez rendre un code python qui tourne sur netbook Jupyter + un mini rapport (format\n",
    "pdf) présentant le travail réalisé, les résultats obtenus et une analyse (intéressante) de ces résultats.\n",
    "\n",
    "### 2.2 Travail à faire\n",
    "\n",
    "1. Charger et transformer les données de façon à ce qu’elles soient reconnues comme des tran-\n",
    "sactions. En pratique on construit un tableau de données binaires.\n",
    "Les instructions Python suivantes permettent de charger le jeu de données retail :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "db = pd.read_csv('retail_dataset.csv',sep=',',header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Utiliser l’algorithme Apriori pour extraire les itemsets fréquents et les maximaux. Vous\n",
    "choisirez un support minimum de 3%.  \n",
    "— Que se passe t’il si on fait varier le seuil du support ?  \n",
    "— Tracer une courbe montrant l’évolution du nombre de itemsets extraits en fonction du support minimum.  \n",
    "\n",
    "3. Nous souhaitons pouvoir filtrer les itemsets selon la présence d’items ou d’un ensemble\n",
    "d’items. Par exemple, quels sont les itemsets qui contiennent le produit ’Eggs’ ? les produits\n",
    "{’Eggs’,’Meat’} ?\n",
    "\n",
    "##### Note\n",
    "Plusieurs solutions s‘offrent à vous pour la recherche d’itemsets répondant à des condi-\n",
    "tions de présence d’items. Vous pouvez par exemple utiliser les opérateurs de comparaison\n",
    "de [pandas.Series](https://pandas.pydata.org/pandas-docs/stable/reference/series.html).\n",
    "\n",
    "4. Utiliser l’algorithme Apriori pour extraire les règles d’association à partir des itemsets\n",
    "fréquents et des itemsets maximaux. Vous choisirez une confiance minimale de 75%. Extraire\n",
    "les règles ayant pour conséquents ’Chesse’.\n",
    "\n",
    "##### Note\n",
    "La fonction association rules() renvoie un objet de type pandas.dataframe contenant\n",
    "les différentes règles d’association, chacune décrite par différentes caractéristiques qui sont\n",
    "l’antécédent, le conséquent, et 7 indicateurs numériques d’évaluation des règles. Il faudra adap-\n",
    "ter l’affichage pour disposer que des informations liées aux mesures support, lift et la confidence.\n",
    "\n",
    "5. Compléter l’analyse des différentes règles d’association extraites via des graphiques permet-\n",
    "tant d’étudier la corrélation entre les trois mesures (lift, confiance et support) d’évaluation\n",
    "des règles.\n",
    "\n",
    "### 3 Références\n",
    "1. [MLxtend : machine learning extensions](http://rasbt.github.io/mlxtend/)\n",
    "2. [Introduction to Market Basket Analysis in Python](https://pbpython.com/market-basket-analysis.html)\n",
    "3. [Association Rule Mining via Apriori Algorithm in Python](https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/)\n",
    "4. [Documentation Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/index.html#api)\n",
    "5. [Documentation MLxtend](http://rasbt.github.io/mlxtend)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}